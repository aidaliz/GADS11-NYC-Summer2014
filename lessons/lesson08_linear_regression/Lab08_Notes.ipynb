{
 "metadata": {
  "name": "",
  "signature": "sha256:7295d40e7739923815f32ad5071134dc486a006fb5baf988c87a4a5230b0df6c"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# More notes about Linear Regression, scikit-learn, etc.\n",
      "\n",
      "Done in a notebook since Github doesn't render LaTeX equations"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Why linear algebra?\n",
      "\n",
      "The reason linear algebra is so pertinent is that it's the best abstraction for solving lots of equations simultaneously. Consider this matrix equation where the final expression is simply a result of matrix multiplication as we explained in class:\n",
      "\n",
      "$$\n",
      "\\mathbf{Y} = \\mathbf{A} \\cdot \\mathbf{X}\n",
      "\\\\\n",
      "\\mathbf{Y} = \\begin{pmatrix}\n",
      "y_1 \\\\ y_2 \\\\ y_3\n",
      "\\end{pmatrix}\n",
      ", \\quad\n",
      "\\mathbf{X} = \\begin{pmatrix}\n",
      "x_1 \\\\ x_2 \\\\ x_3\n",
      "\\end{pmatrix}\n",
      ", \\quad\n",
      "\\mathbf{A} = \\begin{pmatrix}\n",
      "a & b & c \\\\ \n",
      "d & e & f \\\\ \n",
      "g & h & i\n",
      "\\end{pmatrix}\n",
      "\\\\\n",
      "\\begin{pmatrix}\n",
      "y_1 \\\\ y_2 \\\\ y_3\n",
      "\\end{pmatrix}\n",
      "=\n",
      "\\begin{pmatrix}\n",
      "a & b & c \\\\ \n",
      "d & e & f \\\\ \n",
      "g & h & i\n",
      "\\end{pmatrix}\n",
      "\\cdot\n",
      "\\begin{pmatrix}\n",
      "x_1 \\\\ x_2 \\\\ x_3\n",
      "\\end{pmatrix}\n",
      "=\n",
      "\\begin{pmatrix}\n",
      "a x_1 + b x_2 + c x_3 \\\\ \n",
      "d x_1 + e x_2 + f x_3 \\\\ \n",
      "g x_1 + h x_2 + i x_3\n",
      "\\end{pmatrix}\n",
      "$$\n",
      "\n",
      "So ultimately you have a vector of $y$s equaling another vector. In order for the two to be equal, the corresponding elements have to be equal. This means the above equation is equivalent to the following equations:\n",
      "\n",
      "$$\n",
      "y_1 = a x_1 + b x_2 + c x_3 \\\\\n",
      "y_2 = d x_1 + e x_2 + f x_3 \\\\\n",
      "y_3 = g x_1 + h x_2 + i x_3 \\\\\n",
      "$$\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Linear Regression\n",
      "\n",
      "For linear regression, we're trying to find the best line that fits our data. Remember the equation of a line is:\n",
      "\n",
      "$$\n",
      "y = \\alpha_0 + \\alpha_1 x\n",
      "$$\n",
      "\n",
      "Where $\\alpha_0$ and $\\alpha_1$ are the intercept and slope terms, respectively. Instead of the above matrices, we're given features... (Which we pad with ones. You'll see why.):\n",
      "\n",
      "$$\n",
      "\\mathbf{X} = \\begin{pmatrix}\n",
      "1 & x_1 \\\\ 1 & x_2 \\\\ 1 & x_3 \\\\ \\vdots & \\vdots\n",
      "\\end{pmatrix}\n",
      "$$\n",
      "\n",
      "..., outputs:\n",
      "\n",
      "$$\n",
      "\\mathbf{Y} = \\begin{pmatrix}\n",
      "y_1 \\\\ y_2 \\\\ y_3 \\\\ \\vdots\n",
      "\\end{pmatrix}\n",
      "$$\n",
      "\n",
      "...and we apply parameters:\n",
      "\n",
      "$$\n",
      "\\mathbf{\\beta} = \\begin{pmatrix}\n",
      "\\beta_0 \\\\ \\beta_1\n",
      "\\end{pmatrix}\n",
      "$$\n",
      "\n",
      "If we combine them this way:\n",
      "\n",
      "$$\n",
      "\\mathbf{Y} = \\mathbf{X} \\cdot \\mathbf{\\beta}\n",
      "\\\\\n",
      "\\begin{pmatrix}\n",
      "y_1 \\\\ y_2 \\\\ y_3 \\\\ \\vdots\n",
      "\\end{pmatrix}\n",
      "=\n",
      "\\begin{pmatrix}\n",
      "1 & x_1 \\\\ 1 & x_2 \\\\ 1 & x_3 \\\\ \\vdots & \\vdots\n",
      "\\end{pmatrix}\n",
      "\\cdot\n",
      "\\begin{pmatrix}\n",
      "\\beta_0 \\\\ \\beta_1\n",
      "\\end{pmatrix}\n",
      "=\n",
      "\\begin{pmatrix}\n",
      "\\beta_0 + x_1 \\beta_1 \\\\\n",
      "\\beta_0 + x_2 \\beta_1 \\\\\n",
      "\\beta_0 + x_3 \\beta_1 \\\\\n",
      "\\vdots\n",
      "\\end{pmatrix}\n",
      "$$\n",
      "\n",
      "Which of course is equivalent to the equations:\n",
      "\n",
      "$$\n",
      "y_1 = \\beta_0 + x_1 \\beta_1 \\\\\n",
      "y_2 = \\beta_0 + x_2 \\beta_1 \\\\\n",
      "y_3 = \\beta_0 + x_3 \\beta_1 \\\\\n",
      "\\vdots\n",
      "$$\n",
      "\n",
      "So you see when we perform linear regression, we're finding the best $\\beta_0$ and $\\beta_1$ that solve these equations for all the observed data. You also see that the ones are there for the constant, intercept term, $\\beta_0$. The solution, as we gave, is given by:\n",
      "\n",
      "$$\n",
      "\\mathbf{\\beta} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y}\n",
      "$$\n",
      "\n",
      "You can look up a formal proof if you need to. But you see the power hidden here. If we have twenty million records, we don't need to fit them individually; we just understand that $\\mathbf{Y}$ is a vector that's twenty million elements long. If we want to fit using a thousand features instead of just the one $x$, we just understand that $\\mathbf{X}$ is a matrix that's a thousand features wide (plus one for the intercept), and $\\mathbf{\\beta}$ is just as long. But the abstraction remains the same. The code remains the same."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## scikit-learn\n",
      "\n",
      "Linear Regression is familiar to most people, at least in its univariate form. So it's a natural starting point for scikit-learn, which is enormous. Note: sklearn and pandas are not natively compatible. But they're both built on numpy, so that's the interface layer we use.\n",
      "\n",
      "Just about all of the models in sklearn have a common interface.\n",
      "```python\n",
      "model.fit(X[, y])  # train the model using 2D data X and 1D labels y if applicable\n",
      "model.predict(X) or model.transform(X)  # make predictions on new 2D data or transform new 2D data\n",
      "model.score(X[, y])  # evaluate performance\n",
      "```\n",
      "\n",
      "Note that `X` is understood to be 2-dimensional. In numpy terms, this means it must be a 2D ndarray of shape `[n_samples, n_features]`. If you're doing regression with one feature, the `shape` attribute of your data should be `(n_samples, 1)`.\n",
      "\n",
      "This is where doing things in pandas can be a little screwy. `df['column_name']` returns a Series object. `df['column_name'].values` then returns a 1D array with shape `(n,)` (the weird comma is how python indicates this is a tuple with one element and not a number in parentheses).\n",
      "\n",
      "On the other hand, `df[['column_name']]` returns a DataFrame object. That's not a double bracket. It's one bracket for indexing `df`, and another for making a list. This is also valid: `df[['column1', 'column2', 'column3']]`. Thus, `df[['column_name']].values` returns a 2D array with shape `(n, 1)` which is exactly what we want for feeding into sklearn.\n",
      "\n",
      "Why doesn't sklearn just understand 1D arrays? I don't have a good answer to that. But sklearn is overkill if you're trying to fit one-dimensional data anyway. numpy, scipy, pandas, statsmodels, and probably a hundred other libraries have their own implementations of linear regression on one feature.\n",
      "\n",
      "But now that you know this, you're all set for the hundred other models sklearn offers!"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Other points of confusion?\n",
      "\n",
      "If there's anything else that didn't make sense, email me and I'll add to this doc."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}